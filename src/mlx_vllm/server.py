"""FastAPI server application."""

import time
from collections.abc import AsyncGenerator
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from mlx_lm import load

from mlx_vllm.api import adapter_router, router
from mlx_vllm.config import ServerConfig
from mlx_vllm.engine.async_engine import AsyncEngine

# Global config - can be overridden before create_app() is called
_config: ServerConfig | None = None


def get_config() -> ServerConfig:
    global _config
    if _config is None:
        _config = ServerConfig()
    return _config


def set_config(config: ServerConfig) -> None:
    global _config
    _config = config


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """Manage application lifespan - load model and start engine."""
    config = get_config()

    print(f"Loading model: {config.model_path}")
    start = time.time()
    model, tokenizer = load(config.model_path)
    print(f"Model loaded in {time.time() - start:.2f}s")

    # Initialize LoRA layers if configured (for dynamic weight updates)
    if config.lora_rank is not None:
        from mlx_lm.tuner.utils import linear_to_lora_layers

        lora_config = {
            "rank": config.lora_rank,
            "scale": config.lora_scale,
            "dropout": 0.0,
        }
        linear_to_lora_layers(model, config.lora_layers, lora_config)
        model.eval()
        print(f"LoRA initialized: rank={config.lora_rank}, layers={config.lora_layers}")

    engine = AsyncEngine(
        model=model,
        tokenizer=tokenizer,
        max_batch_size=config.max_batch_size,
        default_max_tokens=config.max_tokens,
    )
    await engine.start()
    print("Engine started")

    # Store in app state for routes to access
    app.state.engine = engine
    app.state.config = config
    app.state.model_loaded_at = int(time.time())

    yield

    # Cleanup
    print("Shutting down engine...")
    await engine.stop()


def create_app() -> FastAPI:
    """Create the FastAPI application."""
    app = FastAPI(
        title="MLX-vLLM",
        description="OpenAI-compatible inference server using MLX",
        version="0.1.0",
        lifespan=lifespan,
    )

    # OpenAI-style error handler
    @app.exception_handler(Exception)
    async def openai_error_handler(request: Request, exc: Exception) -> JSONResponse:
        return JSONResponse(
            status_code=500,
            content={
                "error": {
                    "message": str(exc),
                    "type": type(exc).__name__,
                    "code": None,
                }
            },
        )

    # Root-level health check (for load balancers)
    @app.get("/health")
    async def root_health() -> dict:
        engine = app.state.engine
        return {
            "status": "healthy" if engine.is_ready else "not_ready",
            "model": engine.model_id,
        }

    app.include_router(router)
    app.include_router(adapter_router)
    return app


app = create_app()
